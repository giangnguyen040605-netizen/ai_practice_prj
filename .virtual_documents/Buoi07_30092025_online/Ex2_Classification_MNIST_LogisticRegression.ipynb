





import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets, metrics
from sklearn.model_selection import train_test_split
import numpy as np





digits = datasets.load_digits()
n_samples = len(digits.images)
print('The number of samples:', n_samples)


print('Shape of digit samples:', digits.images.shape)





def display_digits(X, Y):
    fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(10, 10))
    fig.suptitle( "Display randomly images of the training data set")
    for i in range(5):
        for j in range(5):
            ind = np.random.randint(X.shape[0])
            tmp = X[ind, :]# .reshape(8, 8)
            ax[i,j].set_title("Label: {}".format(Y[ind]))
            ax[i,j].imshow(tmp, cmap='gray_r')
            plt.setp(ax[i,j].get_xticklabels(), visible=False)
            plt.setp(ax[i,j].get_yticklabels(), visible=False)
    fig.subplots_adjust(hspace=0.5, wspace=0.5)


# Split data into 50% train and 50% test subsets
X_train, X_test, y_train, y_test = train_test_split(digits.images,
                                                    digits.target,
                                                    test_size=0.2,
                                                    random_state=200,
                                                    shuffle=False)
display_digits(X_train, y_train)





from sklearn.decomposition import PCA
# Create a Randomized PCA model that takes two components
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_rpca = pca.fit_transform(digits.data)

# Create a regular PCA model 
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_pca = pca.fit_transform(digits.data)
plt.figure(figsize=(8, 8))
_ = sns.scatterplot(x=reduced_data_pca[:, 0],
                    y=reduced_data_pca[:, 1],
                    hue=digits.target,
                    palette='viridis')
_ = plt.xlabel('PC1')
_ = plt.ylabel('PC2')
_ = plt.title('Visualization of MNIST dataset using PCA', {'fontsize'
: 15}) 





# Flatten the images
X_train = X_train.reshape((len(X_train), -1))
X_test = X_test.reshape((len(X_test), -1))


# ===========================
# Create a classifier here
# Example: This example, we use Logistic Regressor
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty='l2',
                         fit_intercept=True,
                         random_state=2021,
                         solver='lbfgs',
                         max_iter=100,
                         verbose=1,
                         n_jobs=5,)


# Learn the digits on the train subset
clf.fit(X_train, y_train)

# Predict the value of the digit on the test subset
predicted = clf.predict(X_test)


# Show predictions
_, axes = plt.subplots(nrows=1, ncols=6, figsize=(10, 3))
for ax, image, prediction in zip(axes, X_test, predicted):
    ax.set_axis_off()
    image = image.reshape(8, 8)
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    ax.set_title(f'Prediction: {prediction}')





print(f"Classification report for classifier {clf}:\n"
      f"{metrics.classification_report(y_test, predicted)}\n")





from sklearn.metrics import ConfusionMatrixDisplay

plt.figure(figsize=(5, 5))
ax = plt.gca()
disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, ax=ax)
disp.ax_.set_title("Confusion Matrix", fontsize=15)
plt.show()









