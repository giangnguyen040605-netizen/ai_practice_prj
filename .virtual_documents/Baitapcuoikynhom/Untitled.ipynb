








# Import thư viện
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

import matplotlib.pyplot as plt








# Load dữ liệu
df = pd.read_excel("DataCO2.xlsx")


# Chọn năm 2023
df_2023 = df[df['year'] == 2023].copy()


# Tạo target 3 nhóm theo co2 
# Low: <=33%, Medium: 33-66%, High: >66%
co2_quantiles = df_2023['co2'].quantile([0.33, 0.66])
def co2_group(x):
    if x <= co2_quantiles[0.33]:
        return 0  # Low
    elif x <= co2_quantiles[0.66]:
        return 1  # Medium
    else:
        return 2  # High

df_2023['co2_group'] = df_2023['co2'].apply(co2_group)


# Chọn features 
features = ['gdp','co2_per_gdp','population','co2_per_capita','cement_co2','coal_co2',
            'oil_co2','gas_co2','flaring_co2','land_use_change_co2',
            'primary_energy_consumption','temperature_change_from_co2']








# Kiểm tra thông tin dữ liệu
df_2023.info()
df_2023[features].describe()


# Kiểm tra missing values
df_2023.isna().sum()


# Cập nhật lại danh sách features bỏ biến 'gdp' và 'co2_per_gdp'
features = [
    'population', 'co2_per_capita', 'cement_co2', 'coal_co2', 'oil_co2',
    'gas_co2', 'flaring_co2', 'land_use_change_co2',
    'primary_energy_consumption', 'temperature_change_from_co2'
]


# Define X and y
X = df_2023[features].copy()   # Các biến đầu vào đã chọn
y = df_2023['co2_group']       # 3 nhóm theo co2


# train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)








# Pipeline: Imputer + Scaler + Logistic Regression
pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler()),
    ("model", LogisticRegression(max_iter=2000))
])

param_grid_lr = {
    "model__C": [0.01, 0.1, 1, 10],
    "model__solver": ["lbfgs", "liblinear"]
}

grid_lr = GridSearchCV(pipe, param_grid_lr, cv=5, scoring="accuracy", n_jobs=-1)
grid_lr.fit(X_train, y_train)

print("=== Logistic Regression ===")
print("Best params:", grid_lr.best_params_)

# Đánh giá mô hình
y_pred_lr = grid_lr.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))





pipe_dt = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler(with_mean=False)),
    ("model", DecisionTreeClassifier(random_state=42))
])

param_dt = {
    "model__max_depth": [3, 5, 7]
}

grid_dt = GridSearchCV(pipe_dt, param_dt, cv=5, scoring='accuracy')
grid_dt.fit(X_train, y_train)

print("=== Decision Tree ===")
print("Best params:", grid_dt.best_params_)
print("Train CV accuracy:", grid_dt.best_score_)
print("Test accuracy:", grid_dt.score(X_test, y_test))
print(classification_report(y_test, grid_dt.predict(X_test)))





param_grid_dt = {
    'max_depth': [3, 5, 7, 9, None]
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid_dt,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    return_train_score=True
)

grid_dt.fit(X_train, y_train)


import matplotlib.pyplot as plt
import pandas as pd

results_dt = pd.DataFrame(grid_dt.cv_results_)

# Chuyển None → số để hiển thị rõ trên trục
results_dt['param_max_depth_plot'] = results_dt['param_max_depth'].apply(lambda x: 12 if x is None else x)

plt.figure(figsize=(7,5))
plt.plot(results_dt['param_max_depth_plot'], results_dt['mean_train_score'], marker='o', linestyle='--', label='Train Accuracy')
plt.plot(results_dt['param_max_depth_plot'], results_dt['mean_test_score'], marker='o', linestyle='-', label='Test Accuracy')
plt.xlabel("max_depth")
plt.ylabel("Accuracy")
plt.title("Decision Tree: Train vs Test Accuracy for max_depth")
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend()
plt.show()





from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns


pipeline_rf = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),  # xử lý missing values
    ('rf', RandomForestClassifier(random_state=42))
])


param_grid_rf = {
    'rf__n_estimators': [50, 100, 200],
    'rf__max_depth': [3, 5, 7, None],
    'rf__min_samples_split': [2, 5, 10],
    'rf__min_samples_leaf': [1, 2, 4]
}


grid_rf = GridSearchCV(
    pipeline_rf, param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    return_train_score=True
)

grid_rf.fit(X_train, y_train)

print("=== Random Forest ===")
print("Best params:", grid_rf.best_params_)
print("Best CV Accuracy:", grid_rf.best_score_)

y_pred_rf = grid_rf.predict(X_test)
test_acc = accuracy_score(y_test, y_pred_rf)
print("\nTest Accuracy:", test_acc)
print("\nClassification report:\n", classification_report(y_test, y_pred_rf))





# Plot: Fit Time vs n_estimators 
cv_results_df = pd.DataFrame(grid_rf.cv_results_)


summary = cv_results_df.groupby('param_rf__n_estimators').agg(
    mean_test_score=('mean_test_score', 'mean'),
    std_test_score=('std_test_score', 'mean'),
    mean_fit_time=('mean_fit_time', 'mean')
).reset_index()

fig, axs = plt.subplots(1, 2, figsize=(12,4))

# Màu riêng cho từng biểu đồ
color_fit_time = 'tab:blue'
color_accuracy = 'tab:orange'

# Biểu đồ Training Time
axs[0].plot(
    summary['param_rf__n_estimators'],
    summary['mean_fit_time'],
    '-o',
    color=color_fit_time,
    label='Mean Fit Time'
)
axs[0].set_xlabel('Number of Trees')
axs[0].set_ylabel('Mean Fit Time (s)')
axs[0].set_title('Training Time vs Number of Trees')
axs[0].grid(linestyle='--', alpha=0.5)

# Biểu đồ Accuracy
axs[1].errorbar(
    summary['param_rf__n_estimators'],
    summary['mean_test_score'],
    yerr=summary['std_test_score']/np.sqrt(5),
    fmt='-o',
    capsize=4,
    color=color_accuracy,
    label='Mean CV Accuracy'
)
axs[1].set_xlabel('Number of Trees')
axs[1].set_ylabel('Mean CV Accuracy ± 1 SE')
axs[1].set_title('CV Accuracy vs Number of Trees')
axs[1].grid(linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()





# Extract best model from pipeline
rf_best = grid_rf.best_estimator_.named_steps['rf']

# Feature Importance DataFrame
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_best.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importance)


# Lấy số lượng feature
n_features = len(feature_importance)


# Tạo màu gradient với seaborn
colors = sns.color_palette("viridis", n_colors=n_features)

# Vẽ biểu đồ nhiều màu
plt.figure(figsize=(10,6))
plt.barh(
    feature_importance['Feature'],
    feature_importance['Importance'],
    color=colors
)
plt.gca().invert_yaxis()  # Biến quan trọng nhất lên trên
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance - Random Forest")
plt.tight_layout()
plt.show()



