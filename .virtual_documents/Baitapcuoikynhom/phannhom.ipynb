





import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report








# --- Load dữ liệu ---
df = pd.read_excel("DataCO2.xlsx")


# --- Chọn năm 2023 ---
df_2023 = df[df['year'] == 2023].copy()


# --- Tạo target 3 nhóm theo co2 ---
# Low: <=33%, Medium: 33-66%, High: >66%
co2_quantiles = df_2023['co2'].quantile([0.33, 0.66])
def co2_group(x):
    if x <= co2_quantiles[0.33]:
        return 0  # Low
    elif x <= co2_quantiles[0.66]:
        return 1  # Medium
    else:
        return 2  # High

df_2023['co2_group'] = df_2023['co2'].apply(co2_group)








# --- Chọn features và drop cột toàn NaN ---
features = ['gdp','co2_per_gdp','population','co2_per_capita','cement_co2','coal_co2',
            'oil_co2','gas_co2','flaring_co2','land_use_change_co2',
            'primary_energy_consumption','temperature_change_from_co2',
            'co2_growth_abs','co2_growth_prct']

X = df_2023[features].copy()
X = X.dropna(axis=1, how='all')  # drop cột toàn NaN
y = df_2023['co2_group']


# --- Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)


# Impute median + chuẩn hóa
imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)

X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)








from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report


# Model
lr = LogisticRegression(max_iter=200)

# Grid params
param_grid_lr = {
    'C':[0.01,0.1,1,10],
    'solver':['lbfgs','liblinear']
}

grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)
grid_lr.fit(X_train, y_train)

print("=== Logistic Regression ===")
print("Best params:", grid_lr.best_params_)
print("Train accuracy:", grid_lr.best_score_)

y_pred_lr = grid_lr.predict(X_test)
print(classification_report(y_test, y_pred_lr))





from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)

param_grid_dt = {
    'max_depth':[3,5,7,None],
    'min_samples_split':[2,5,10],
    'min_samples_leaf':[1,2,4]
}

grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)
grid_dt.fit(X_train, y_train)

print("=== Decision Tree ===")
print("Best params:", grid_dt.best_params_)
print("Train accuracy:", grid_dt.best_score_)

y_pred_dt = grid_dt.predict(X_test)
print(classification_report(y_test, y_pred_dt))


import matplotlib.pyplot as plt
import pandas as pd


# Lấy kết quả từ GridSearchCV
cv_results_dt = pd.DataFrame(grid_dt.cv_results_)

# Kiểm tra các cột còn lại
print(cv_results_dt.columns)


grid_dt = GridSearchCV(
    dt, 
    param_grid_dt, 
    cv=5, 
    scoring='accuracy', 
    n_jobs=-1,
    return_train_score=True,   # <--- bắt buộc để lấy mean_train_score
    verbose=1
)
grid_dt.fit(X_train, y_train)


import pandas as pd

cv_results_dt = pd.DataFrame(grid_dt.cv_results_)


plot_df = cv_results_dt[
    (cv_results_dt['param_min_samples_split'] == 2) &
    (cv_results_dt['param_min_samples_leaf'] == 1)
].copy()

# Thay None bằng số để vẽ trục x
plot_df['param_max_depth_plot'] = plot_df['param_max_depth'].apply(lambda x: 12 if x is None else x)


import matplotlib.pyplot as plt

n_splits = 5  # cv=5

fig, ax = plt.subplots(figsize=(7,5))

# Training line
ax.errorbar(
    plot_df['param_max_depth_plot'],
    plot_df['mean_train_score'],
    yerr=plot_df['std_train_score']/n_splits**0.5,
    label='Mean ± 1 SE training scores',
    marker='o',
    linestyle='--',
    color='blue'
)

# Testing line
ax.errorbar(
    plot_df['param_max_depth_plot'],
    plot_df['mean_test_score'],
    yerr=plot_df['std_test_score']/n_splits**0.5,
    label='Mean ± 1 SE testing scores',
    marker='o',
    linestyle='-',
    color='red'
)

ax.set_xlabel('max_depth')
ax.set_ylabel('Accuracy')
ax.set_title('Decision Tree: Train vs Test Accuracy for max_depth')
ax.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()





from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    'n_estimators':[50,100,200],
    'max_depth':[3,5,7,None],
    'min_samples_split':[2,5,10],
    'min_samples_leaf':[1,2,4]
}

grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train, y_train)

print("=== Random Forest ===")
print("Best params:", grid_rf.best_params_)
print("Train accuracy:", grid_rf.best_score_)

y_pred_rf = grid_rf.predict(X_test)
print(classification_report(y_test, y_pred_rf))


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


# Chuyển kết quả GridSearchCV thành DataFrame
cv_results_df = pd.DataFrame(grid_rf.cv_results_)

# Lọc ra các giá trị mean_test_score và mean_fit_time theo n_estimators
# Tính trung bình test score và fit_time theo n_estimators
summary = cv_results_df.groupby('param_n_estimators').agg(
    mean_test_score=('mean_test_score', 'mean'),
    std_test_score=('std_test_score', 'mean'),  # tính trung bình std
    mean_fit_time=('mean_fit_time', 'mean')
).reset_index()


# Vẽ biểu đồ
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,4))

# Biểu đồ fit time
axs[0].plot(summary['param_n_estimators'], summary['mean_fit_time'], '-o', color='blue')
axs[0].set_xlabel('Number of Trees')
axs[0].set_ylabel('Mean Fit Time (s)')
axs[0].set_title('Training Time vs Number of Trees')

# Biểu đồ accuracy (hoặc scoring metric)
axs[1].errorbar(
    summary['param_n_estimators'], 
    summary['mean_test_score'], 
    yerr=summary['std_test_score']/np.sqrt(5),  # 5-fold CV
    fmt='-o', color='green', capsize=5
)
axs[1].set_xlabel('Number of Trees')
axs[1].set_ylabel('Mean CV Accuracy ± 1 SE')
axs[1].set_title('CV Accuracy vs Number of Trees')

plt.tight_layout()
plt.show()





import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Lấy mô hình Random Forest tốt nhất sau GridSearchCV
rf_best = grid_rf.best_estimator_


# Tạo dataframe feature importance
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_best.feature_importances_
})


# Sắp xếp giảm dần
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)


# In bảng
print(feature_importance)


# Lấy số lượng feature
n_features = len(feature_importance)


# Tạo màu gradient với seaborn
colors = sns.color_palette("viridis", n_colors=n_features)


# Vẽ biểu đồ nhiều màu
plt.figure(figsize=(10,6))
plt.barh(
    feature_importance['Feature'],
    feature_importance['Importance'],
    color=colors
)
plt.gca().invert_yaxis()  # Biến quan trọng nhất lên trên
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.title("Feature Importance - Random Forest")
plt.tight_layout()
plt.show()



